\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{enumerate}
\usepackage{fancyhdr}
%\usepackage{minted}
\usepackage{graphicx}
\usepackage{array}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{morefloats}
\usepackage{amsmath}
\usepackage{titlesec}
\usepackage[spanish]{babel}
\usepackage[spanish]{algorithm2e}
\usepackage[table]{xcolor}
\usepackage{anysize}
\usepackage[section]{placeins}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsmath}


\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=blue
}

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{
	\normalfont \normalsize 
	\textsc{\textbf{Metaheurísticas (2016-2017)} \\ Grado en Ingeniería Informática \\ Universidad de Granada} \\ [25pt] % Your university, school and/or department name(s)
	\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
	\huge Práctica 2.A: Técnicas de Búsqueda basadas en Poblaciones para el Problema de la Asignación Cuadrática  \\ % The assignment title
	\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Germán González Almagro} % Nombre y apellidos

\marginsize{3cm}	% margen izquierdo
{3cm}				% margen derecho
{1.2cm}				% margen superior
{1.5cm} 			% margen inferior

%\titleformat{\section}{\normalfont\LARGE}{}{0pt}{\bfseries\LARGE}

\begin{document}
	
	\date{\today}
	
	\maketitle
	
	\begin{figure}[!h]
		\centering
		\includegraphics[scale=0.35]{LogoUGR.png} 
	\end{figure}
	
	\noindent DNI: 76593910T\\Correo Electrónico: germang.almagro@correo.ugr.es\\Subgrupo de prácticas 3. Horario: Lunes de 5:30 a 7:30
	
	\vspace{1cm}
	
	\noindent Algoritmos Considerados: Greedy, búsqueda local (BL), búsqueda multiarranque básica (BMB), enfriamiento simulado (ES), GRASP, búsqueda local reiterada (ILS), híbrido ILS-ES.
	
	
	\newpage
	
	\tableofcontents
	
%	\listoffigures
	
	\listoftables
	
	\newpage
	
\section{Descripción del problema}

	\noindent El problema de la asignación cuadrática o QAP (Quadratic Assignment Problem) es un problema de optimización combinatoria que pertenece a la clase de problemas NP-Completos. Consiste en determinar la asignación óptima de $n$ unidades funcionales a $n$ localizaciones, conociendo el flujo que circula entre cada unidad funcional y la distancia entre las localizaciones.\\
	
	\noindent Para representar este problema de forma que sea computable, consideraremos las matrices $F$ y $D$, que almacenarán el flujo entre unidades y la distancia entre localizaciones respectivamente. De esta manera el flujo entre las unidades $i$ y $j$ es $F_{ij}$ y la distancia entre las localizaciones $k$ y $l$ es $D_{kl}$; el coste de asignar la unidad $i$ a la localización $k$ y la unidad $j$ a la localización $l$ es $F_{ij} \times D_{kl}$.\\
	
	\noindent Conociendo la representación del problema, podemos plantearlo como:
	
	$$ QAP = min \left( \sum_{i=1}^{n}\sum_{j=1}^{n}F_{ij} \times D_{S(i)S(j)} \right)\; t.q.\; S \in \prod_{N} $$
	
\section{Métodos de resolución del problema}
	
	\noindent Dado que es computacionalmente costoso resolver este problema de forma óptima mediante algoritmos puramente deterministas, emplearemos diversas técnicas heurísticas para obtener una solución que, aunque no es óptima, es de calidad. En este caso emplearemos métodos basados en trayectorias como pueden ser el Enfriamiento Simulado (ES), Búsqueda Multiarranque Básica (BMB), Búsqueda Local Reiterada (ILS), GRASP y un híbrido que incluye ES e ILS. Además consideraremos algunos de los métodos de resolución ya desarrollados en la práctica anterior, Búsqueda Local o la resolución mediante algoritmo Greedy.\\
	
	\noindent Aunque las diferentes técnicas de resolución presentan diferencias significativas, también presentan similitudes; una de ellas es la representación del problema. Las soluciones al problema estarán representadas por una estructura que contendrá un vector que almacena la permutación propuesta como solución, en el que los índices representan las unidades funcionales y el contenido las localizaciones asociadas, además contendrá un dato de tipo \texttt{int} que representa el valor asociado a la permutación propuesta. Entenderemos por dimensión del problema del número de unidades funcionales o localizaciones que lo conforman.
	
	\noindent De igual forma que las diferentes técnicas de resolución comparten el modelo de representación, los algoritmos que las implementan utilizan procesos comunes que pueden ser definidos de forma modular como sigue:

\clearpage
	
	\subsection{Función objetivo}
	
		\noindent La función objetivo es la encargada de dar valor al campo de tipo \texttt{int} asociado a una solución, para ello, recorre la matriz de distancias y de flujo realizando las operaciones pertinentes sobre un acumulador.
		
		\begin{algorithm}
			\KwIn{Dada una solcuión $S$}
			\textbf{function} $CalcularSolucionNumerica(\&S)$
			\Begin{
				$SNumerica \leftarrow 0$\\
				\For{$i \leftarrow 0$ \KwTo $Dimension$}{
					\vspace{0.2cm}
					\For{$j \leftarrow 0$ \KwTo $Dimension$}{
						\vspace{0.2cm}
						$SNumerica \leftarrow SNumerica + MatrizFlujo_{ij} \times$\\ $MatrizDistancias_{S_i,S_j}$		
					}
				}
				$S.SolucionNumerica \leftarrow SNumerica$
			}
			
		\end{algorithm}
	
	\subsection{Factorizaciones de la función objetivo}
	
		\noindent Es posible reducir el esfuerzo computacional realizado al evaluar un individuo de forma que, conociendo el estado del mismo, a saber, permutación que almacena y valor numérico de la misma, factorizamos el cálculo del coste de forma que el orden de complejidad del algoritmo que lo calcula se reduce de $\mathcal{O}(n^2)$ a $\mathcal{O}(n)$.			
		
		\begin{algorithm}
			\KwIn{Dados $s$ y $r$ las posiciones a intercambiar y $\pi$ la solución}
			\textbf{function} $ChequearMovimiento(s,\; r,\; \pi)$
			\Begin{
				$Incremento \leftarrow 0$\\
				\For{$k \leftarrow 0$ \KwTo $Dimension$}{
					\vspace{0.2cm}
					\If{$k \neq s$ $\&\&$ $k \neq r$}{
						
						$Incremento \leftarrow Incremento + F_{rk}(D_{\pi(s),\pi(k)} - D_{\pi(r),\pi(k)}) +$\\
						$F_{sk}(D_{\pi(r),\pi(k)} - D_{\pi(s),\pi(k)}) +$\\
						$F_{kr}(D_{\pi(k),\pi(s)} - D_{\pi(k),\pi(r)}) +$\\
						$ F_{ks}(D_{\pi(k),\pi(r)} - D_{\pi(k),\pi(s)})$
						
					}		
				}
				
				\KwRet $Incremento$
			}
			
		\end{algorithm}
		
		\noindent Dado que, tanto el orden de complejidad como el espacio de trabajo de la función objetivo y su versión factorizada no es el mismo, es lógico pensar que no deben contabilizar de igual forma para la obtención del número total de llamadas a la función objetivo que realiza un algoritmo. Como ya hemos visto, mientras que la orden de complejidad de la función objetivo es $\mathcal{O}(n^2)$, el de su versión factorizada es de orden $\mathcal{O}(n)$, concretamente $\mathcal{O}(4n)$, por tanto para completar una llamada a la función objetivo empleando únicamente llamadas a la función factorizada será necesario realizar $\frac{N}{4}$ llamadas a la misma.\\
		
		\noindent Por otra parte, dada una solución parcial, es decir, una solución en la que no todos los elementos de la permutación han sido asignados, podemos calcular el incremento en el coste de la solución que supone asignar una nueva unidad a una nueva localización. Para ello es necesario conocer la unidad y la localización a añadir, así como los elementos de la permutación que deben ser considerados. A continuación se muestra el pseudocódigo que describe esta idea:
		
		\begin{algorithm}
			\KwIn{Dados $u$ la unidad a asignar, $l$ la localización a asignar, $\pi$ la solución parcial y $asig$ la posiciones a tener en cuenta en $\pi$}
			\textbf{function} $ChequearAdicion(u,\; l,\; \pi,\;asig)$
			\Begin{
				$Incremento \leftarrow 0$\\
				\For{$i \leftarrow 0$ \KwTo $asig.tamanio()$}{
					
					$Incremento \leftarrow Incremento + F_{asig[i],u}\times D_{\pi(asig[i]),l}$\\
					$Incremento \leftarrow Incremento + F_{u,asig[i]}\times D_{l,\pi(asig[i])}$\\
						
							
				}
				
				\KwRet $Incremento$
			}
			
		\end{algorithm}
		
	\subsection{Generación de soluciones aleatorias}
	
		\noindent Los algoritmos no constructivos (algoritmos de mejora) necesitan una solución inicial aleatoria sobre la que comenzar a trabajar, para generar las soluciones aleatorias implementamos el siguiente método:\\
		
		\begin{algorithm}
			
			\textbf{function} $GenerarSolucionAleatoria(\&Solucion)$
			\Begin{
				
				$Solucion \leftarrow \emptyset$\\
				\vspace{0.2cm}
				\For{$i \leftarrow 0$ \KwTo $Dimension$}{
					$Solucion[i] \leftarrow i$\\
					
				}
				
				\texttt{\#Barajamos el vector que contiene la permutación solución}\\
				\For{$i \leftarrow 0$ \KwTo $Dimension$}{
					
					$Rand \leftarrow AleatorioEntre(i,\;Dimension)$\\
					$Solucion.Intercambiar(Rand,\; i)$\\
					
				}
				
				$CalcularSolucionNumerica(Solucion)$\\
			}
			
			
		\end{algorithm}
		
\clearpage

\section{Algoritmo de comparación: Algoritmo Greedy}

	\noindent Los algoritmos voraces proporcionan soluciones a problemas en tiempo mínimo a costa de una pérdida de calidad en la solución. En este caso el algoritmo voraz consiste en seleccionar iterativamente las unidades funcionales que maximicen el flujo y situarlas en las localizaciones más céntricas, es decir, en aquellas localizaciones que minimicen la distancia. Para ello basta con almacenar en dos vectores la suma por filas de las matrices de flujo y distancia respectivamente, y seleccionar de entre ellos de la forma descrita.
	
	\begin{algorithm}[!h]
		
		\textbf{function} $Greedy()$
		\Begin{
			
			$DistSumV \leftarrow \emptyset\; FluxSumV \leftarrow \emptyset$\\
			\texttt{\#Calcular los vectores que almacenan la suma}\\
			\For{$i \leftarrow 0$ \KwTo $Dimension$}{
				
				$FSum \leftarrow 0\;\;DSum \leftarrow 0$\\
				
				\For{$j \leftarrow 0$ \KwTo $Dimension$}{
					
					$FSum \leftarrow FSum + MatrizFlujo[i][j]$\\
					$DSum \leftarrow DSum + MatrizDistancia[i][j]$\\
					
				}
				
				$FluxSumV[i] \leftarrow FSum$\\
				$DistSumV[i] \leftarrow DSum$
				
				
			}
			
			\texttt{\#Asignar unidades a localizaciones}\\
			\For{$i \leftarrow 0$ \KwTo $Dimension$}{
				
				$MaxFluxVal \leftarrow -1$\\
				$MinDistVal \leftarrow \infty$\\
				
				\For{$j \leftarrow 0$ \KwTo $Dimension$}{
					
					\texttt{\#Actualizar mejor flujo y unidad}\\
					\If{$Encontrado flujo mayor$}{
						$MaxFluxVal \leftarrow FluxSumV[i]$\\
						$MaxFluxValInd \leftarrow i$\\	
						
					}
					\texttt{\#Actualizar mejor distancia y localización}\\
					\If{$Encontrada distanci menor$}{
						$MinDistVal \leftarrow DistSumV[i]$\\
						$MinDistValInd \leftarrow i$\\	
						
					}
					
				}
				\texttt{\#Asignar localización a unidad}
				$Solucion[MaxFluxValInd] \leftarrow MinDistValInd$\\
				\texttt{\#Marcar unidad y localización como procesados}\\
				$FluxSumV[MaxFluxValInd] \leftarrow -1$\\
				$DistSumV[MinDistValInd] \leftarrow \infty$\\
				
				
			}
			
			
		}
		
	\end{algorithm}
	
\clearpage
		
\section{Algoritmo de Búsqueda Local}
		
	\noindent En esta ocasión, emplearemos, para la implementación de la búsqueda local, un vector que nos permitirá ``guiar'' el proceso de búsqueda de forma que evitamos explorar ramas que sabemos que no conducen a una buena solución, el vector Don't look bits (DLB). El esquema general consiste en generar vecinos de una solución de manera aleatoria, para no dar prioridad a unos sobre otros, de forma que la solución actual será reemplazada por el primer vecino que proporcione una mejora a la solución. A continuación se muestra el pseudocódigo del método descrito. (Suponer que las operaciones se realizan sobre una interna a la clase).
		
	\begin{algorithm} [!h]
		
		\textbf{function} $BusquedaLocal(MaxLlamadas)$
		\Begin{
			
			$Intercamb \leftarrow Solucion \leftarrow PermutacionAleatoria$\\
			
			
			\While{$Se\; ha\; producido\; mejora\; \&\&\; Llamadas\; <\; MaxLlamada$}{
				
				$BarajarVector(Intercamb)$\\
				$DLB \leftarrow \{0\}$	
				
				\For{$(i \leftarrow 0$; $\;i<Dimension\; \&\&\; No Mejora$; $\;i++)$}{
					
					\uIf{$DLB[i] == 0$}{
						
						\For{$(i \leftarrow 0$; $\;i<Dimension\; \&\&\; No Mejora$; $\;i++)$}{
							
							$Incremento \leftarrow ChequearMovimiento(Intercamb[i],Intercamb[j])$\\
							$Llamadas \leftarrow Llamadas + 1/(Dimension/4)$
							
							\uIf{$Incremento < 0$}{
								
								$Individuo \leftarrow Vecino$\\
								$Llamadas \leftarrow Llamadas + 1/(Dimension/4)$\\
								$DLB[Intercamb[i]] \leftarrow DLB[Intercamb[j]] \leftarrow 0$
								
							}
							\Else{
								
								$DLB[Intercamb[i]] \leftarrow 1$
								
							}
							
							
						}
						
					} 
					
				}
				
			}
			
			
		}
		
	\end{algorithm}
		
\clearpage
		
\section{Algoritmo de Enfriamiento simulado}

	\noindent Enfriamiento simulado es una técnica de exploración del espacio de soluciones basada en trayectorias. Introduce el concepto de plan de enfriamiento, que imita el proceso de enfriamiento descrito por los modelos físicos para considerar soluciones peores a la actual que le puedan llevar a un mejor óptimo que el que alcanza la búsqueda local básica.\\
	
	\noindent Es necesario entonces describir un plan de enfriamiento para el funcionamiento de este algoritmo, en este caso emplearemos el esquema de enfriamiento de Cauchy modificado:
	
	$$ T_{k+1} = \frac{T_k}{1+\beta T_k} \;\;\;\; \beta = \frac{T_0 - T_f}{M T_0 T_f} \;\;\;\; T_0 = \frac{\mu C(S_0)}{-ln(\phi)}  $$
	
	\noindent Donde $M$ es el número de enfriamientos a realizar, $T_0$ es la temperatura inicial y $T_f$ es la temperatura final, que tendrá un valor cercano a 0. A continuación se describe en pseudocódigo la implementación de esta idea:
	
	\begin{algorithm}
		
		\textbf{function} $EnfiramientoSimulado(...)$
		\Begin{
			\texttt{\#Inicalizamos la solución y el vector de intercambios}\\
			$Solucion \leftarrow PermutacionAleatoria$\\
			$Intercambiador \leftarrow PermutacionAleatoria$\\
			\texttt{\#Inicalizamos los parámetros del enfriamiento}\\
			$T = (\mu * C(Solucion))/(-log(\phi))$\\
			$\beta = (T - T_{fin})/(M * T * T_{fin})$\\
			\vspace{0.2cm}
			\While{$T > T_{fin}\;\; \&\& \;\;successes > 0\;\; \&\& \;\;K <= M$}{
				
				$incremento \leftarrow generados \leftarrow 0$\\
				$Barajar(Intercambiador)$\\
				
				\For{$i \leftarrow 0$ \KwTo $Dimension$}{
					
					\For{$j \leftarrow i+1$ \KwTo $Dimension$}{
						
						\texttt{\#Generamos un vecino}\\
						$a \leftarrow intercambiador[i]; \;\; b \leftarrow intercambiador[j]$\\
						$incremento \leftarrow ChaquearMovimiento(a,b)$\\
						$generados \leftarrow generados + 1$\\
						\texttt{\#Aceptamos el vecino si es mejor o si lo indica el proceso de enfriamiento}\\
						\If{$incremento < 0\;\; || \;\;Rand() \le e^{(-incremento/T)}$}{
							
							$AplicarMovimiento(a\;,b,\;incremento)$\\
							$exitos \leftarrow exitos + 1$\\
							
						}
					}	
				}
				
				\texttt{\#Actualizamos la temperatura}\\
				$T \leftarrow \frac{T}{1 + \beta T}$\\
			}
		}
		
	\end{algorithm}
	
	Debemos tener en cuenta que los dos bucles internos al \texttt{while} también se detendrán si se alcanza el número máximo de vecinos permitidos, contabilizados por la variable $generados$ o el número máximo de actualizaciones de solución, contabilizados por la variable $exitos$.
	
\section{Algoritmo de Búsqueda Multiarranque Básica}

	\noindent El algoritmo de búsqueda multiarranque básica consiste en lanzar el algoritmo de búsqueda local sobre un número $N$ de soluciones generadas de forma aleatoria, de esta forma es posible encontrar mejores soluciones que con una única búsqueda local. A continuación se muestra el pseudocódigo que implementa esta idea (la función $C$ es la función de coste):
	
	\begin{algorithm}
		
		\textbf{function} $BMB(MaxIters,\;MaxLlamadasLS)$
		\Begin{
			\texttt{\#Inicializamos la mejor solución de forma aleatoria}\\
			$MejorSolucion \leftarrow GenerarSolucionAleatoria()$\\
			\vspace{0.2cm}
			\For{$i \leftarrow 0$ \KwTo $MaxIters$}{
				\texttt{\#Generamos una nueva solución de forma aleatoria}\\
				$SlcnAleatoria \leftarrow GenerarSolucionAleatoria()$\\
				\texttt{\#Lanzamos BL sobre la nueva solución aleatoria}\\
				$NuevaSlcn \leftarrow BusquedaLocal(SlcnAleatoria,\;MaxLlamadasLS)$\\
				
				\texttt{\#Actualizamos la mejor solución si fuese necesario}\\
				\If{$C(NuevaSlcn) < C(MejorSolucion)$}{
					$MejorSolucion \leftarrow NuevaSlcn$\\
				}
				
			}
			
			\KwRet{$MejorSolucion$}
			
		}
		
	\end{algorithm}
	
	\noindent El proceso de generación de soluciones aleatorias ha sido descrito en anteriores secciones de este documento.
	
\clearpage

\section{Algoritmo GRASP}

	\noindent El procedimiento GRASP consiste en lanzar el algoritmo de Búsqueda Local sobre soluciones generadas mediante un procedimiento greedy aleatorizado que introduce diversidad en la exploración del espacio de soluciones. Este procedimiento de generación de soluciones greedy aleatorizadas consta de dos etapas, en la primera se asignan dos unidades a dos localizaciones conjuntamente, estas serán seleccionadas aleatoriamente de entre la lista de mejores candidatos correspondientes a cada una, en la etapa dos se asigna una unidad a una localización seleccionadas aleatoriamente de entre la lista de aquellas asignaciones factibles que provocan el menor incremento en el coste de la solución.\\
	
	\noindent A continuación se muestra el pseudocódigo correspondiente a la obtención de las listas de candidatos de la primera etapa, y el pseudocódigo correspondiente al procedimiento de generación de soluciones greedy aleatorizadas:
	
	\begin{algorithm}
		
		\textbf{function} $ListasCostes(\&Unidades,\;\&Locs, \;\alpha)$
		\Begin{
			\texttt{\#Inicializamos los límites}\\
			$UnidadMin \leftarrow LocMin \leftarrow \infty$\\
			$UnidadMax \leftarrow LocMax \leftarrow -\infty$\\
			\texttt{\#Buscamos los máximos y mínimos en las unidades y localizaciones}\\
			\For{$i \leftarrow 0$ \KwTo $Dimension$}{
				
				\If{$Unidades[i] > UnidadMax$}{$UnidadMax \leftarrow Unidades[i]$}
				\If{$Unidades[i] < UnidadMin$}{$UnidadMin \leftarrow Unidades[i]$}
				\If{$Locs[i] > LocMax$}{$LocMax \leftarrow Locs[i]$}
				\If{$Locs[i] < LocMin$}{$LocMin \leftarrow Locs[i]$}
			
			}
			\texttt{\#Calculamos los umbrales $\mu$ para las listas de candidatos}\\
			$\mu_{u} = UnidadMax - \alpha (UnidadMax-UnidadMin)$\\
			$\mu_{l} = LocMin - \alpha (LocMax-LocMin)$\\
			\texttt{\#Inicilizamos las listas de mejores candidatos}\\
			$MejoresUnds \leftarrow MejoresLocs \leftarrow \emptyset$\\
			\texttt{\#Añadimos unidades y localizaciones a las listas de mejores candidatos según los umbrales $\mu_{u}$ y $\mu_{l}$}\\
			\For{$i \leftarrow 0$ \KwTo $Dimension$}{
				
				\If{$Unidades[i] >= \mu_{u}$}{
					$MejoresUnds.Add(pareja(i,Unidades[i]))$
				
				}
				
				\If{$Localzcns[i] <= \mu_{l}$}{
					$MejoresLocs.Add(pareja(i,Locs[i]))$
					
				}
				
			}
			
			\KwRet $(MejoresUnds,\;MejoresLocs)$
		}
		
		
	\end{algorithm}
	

	\begin{algorithm}
		
		\textbf{function} $CalcularGreedyAleatorizado(\alpha)$
		\Begin{
			$DistSumV \leftarrow FluxSumV \leftarrow \emptyset$\\
			\texttt{\#Calculamos los vectores que almacenan la suma de igual forma que en el Greedy}\\
			$FSum \leftarrow SumaFlujos;\;DSum \leftarrow SumaDistancias$\\
			\texttt{\#Inicalizamos el vector de posibles asignaciones (bucle doble)}\\
			\For{$i,j \leftarrow 0$ \KwTo $Dimension$}{
				$AsigsFactibles.Add(pareja(i,j))$
			}
			
			$(MejoresUnds, MejoresLocs) \leftarrow ListasCostes(FSum,\;DSum,\;\alpha)$\\
			\texttt{\#Asignamos las dos primeras localizaciones a unidades de forma aleatoria (R1 $\ne$ R3 y R2 $\ne$ R4)}\\
			$Solucion[MejoresUnds[R1].first] \leftarrow MejoresLocs[R2].first$\\
			$Solucion[MejoresUnds[R3].first] \leftarrow MejoresLocs[R4].first$\\
			$Asigs.Add(MejoresUnds[R1].first);\;Asigs.Add(MejoresUnds[R3].first)$\\
			\texttt{\#Tachamos de entre las asignaciones posibles aquellas relacionadas con las asignaciones realizadas}\\
			\For{$i \leftarrow 0$ \KwTo $AsigsFactibles.tamanio()$}{
				\If{$Ya \; asignada\; AsigsFactibles[i]$}{
					$AsigsFactibles[i] \leftarrow -1$
				}
				$ValorAsig.Add(\infty)$\texttt{\#Inicializamos el vector de costes de asignaciones}\\
			}
			
			\For{$k \leftarrow 0$ \KwTo $Dimension - 2$}{
				
				\For{$i \leftarrow 0$ \KwTo $AsigsFactibles.tamanio()$}{
					
					\If{$No \; tachada \; AsigsFactibles[i]$}{
						
						$ValorAsig[i] \leftarrow ChequearAdicion(AsigsFactibles[i]\; Solucion,\;Asigs)$\\
						
						\If{$ValorAsig[i] < min$}{$min \leftarrow ValorAsig[i]$}
						\If{$ValorAsig[i] > max$}{$max \leftarrow ValorAsig[i]$}
					}
					
				}
				
				$\mu \leftarrow min + \alpha * (max - min)$\texttt{\#Calculamos el umbral}\\
				\texttt{\#Obtenemos la lista de candidatos}\\
				$Candidatos \leftarrow ObtenerCandidatos(ValorAsig,\; \mu)$\\
				$(Unidad,\;Loc) \leftarrow AsigsFactibles[Candidatos[R1]]$\texttt{\#R1 entero aleatorio}\\
				\texttt{\#Asignamos la unidad y localización seleccionadas}\\
				$Solucion[Unidad] \leftarrow Loc;\;Asigs.Add(Unidad)$\\
				\texttt{\#Tachamos las asignaciones que dejan de ser factibles}\\
				\For{$i \leftarrow 0$ \KwTo $AsigsFactibles.tamanio()$}{
					\If{$Ya \; asignada\; AsigsFactibles[i]$}{
						$AsigsFactibles[i] \leftarrow -1$
					}
				}
			}
			$CalcularCoste(Solucion);\;\;$ \KwRet $Solucion$
			
		}
		
	\end{algorithm}

	\noindent Finalmente se muestra el pseudocódigo correspondiente al procedimiento que emplea los métodos anteriormente descritos para obtener una solución (la función $C$ es la función de coste):
	
	\begin{algorithm}
		
		\textbf{function} $GRASP(MaxIters,\;MaxLlamadasLS,\;\alpha)$
		\Begin{
			\texttt{\#Inicializamos la mejor solución de forma aleatoria}\\
			$MejorSolucion \leftarrow GenerarSolucionAleatoria()$\\
			\vspace{0.2cm}
			\For{$i \leftarrow 0$ \KwTo $MaxIters$}{
				\texttt{\#Generamos una nueva solución greedy aleatorizada}\\
				$SlcnGreedy \leftarrow CalcularGreedyAleatorizado(\alpha)$\\
				\texttt{\#Lanzamos BL sobre la nueva solución obtenida}\\
				$NuevaSlcn \leftarrow BusquedaLocal(SlcnGreedy,\;MaxLlamadasLS)$\\
				
				\texttt{\#Actualizamos la mejor solución si fuese necesario}\\
				\If{$C(NuevaSlcn) < C(MejorSolucion)$}{
					$MejorSolucion \leftarrow NuevaSlcn$\\
				}
				
			}
			
			\KwRet{$MejorSolucion$}
			
		}
		
	\end{algorithm}
	
\clearpage

\section{Algoritmo de Búsqueda Local Reiterada}

	\noindent El algoritmo de Búsqueda Local Reiterada consiste en lanzar búsqueda local sobre soluciones generadas aplicando un operador de mutación fuerte a la mejor solución encontrada por el algoritmo hasta el momento; de esta forma se evita la localidad en cierta medida. El operador de mutación consiste en seleccionar un fragmento fijo de la permutación que constituye la solución y desordenarlo de manera aleatoria, del tal manera que la solución resultante sea diferente a la original. A continuación se muestra el pseudocódigo que describe esta idea:
	
	\begin{algorithm}
		
		\textbf{function} $OpMutSblstAleatoria(\&Solucion,\;FraccionSublista)$
		\Begin{
			\texttt{\#Calculamos el tamaño de la sublista}\\
			$TamanioSublista \leftarrow Dimension/FraccionSublista$\\
			$Sublista \leftarrow \emptyset$\\
			\texttt{\#Establecemos la posición de inicio de la sublista de forma aleatoria}\\
			$PosInicio \leftarrow AleatorioEntre(0,Dimension)$\\
			
			\texttt{\#Extraemos la sublista}\\
			\For{$i \leftarrow 0$ \KwTo $TamanioSublista$}{
				
				$Sublista[i] \leftarrow Solucion[(i + PosInicio)\%Dimension]$
				
			}
			
			\texttt{\#Desordenamos la sublista}\\
			$BarajarVector(\&Sublista)$
			
			\texttt{\#Reinsertamos la sublista}\\
			\For{$i \leftarrow 0$ \KwTo $TamanioSublista$}{
				
				$Solucion[(i + PosInicio)\%Dimension] \leftarrow Sublista[i]$	
			}
			
			\texttt{\#Recalculamos el coste de la solución}\\
			$CalcularCoste(\&Solucion)$
			
		}
		
	\end{algorithm}
		
	\noindent A continuación se muestra el código que describe el proceso de búsqueda local reiterada (la función $C$ es la función de coste):
		
	\begin{algorithm}
		
		\textbf{function} $ILS(MaxIters,\;MaxLlamadasLS,\;FraccSublista)$
		\Begin{
			\texttt{\#Inicializamos la mejor solución de forma aleatoria}\\
			$MejorSolucion \leftarrow GenerarSolucionAleatoria()$\\
			\vspace{0.2cm}
			\For{$i \leftarrow 0$ \KwTo $MaxIters$}{
				\texttt{\#Generamos una nueva solución de forma aleatoria}\\
				$SlcnMutada \leftarrow OpMutSblstAleatoria(\&MejorSolucion,\;FraccSublista)$\\
				\texttt{\#Lanzamos BL sobre la nueva solución obtenida con el operador de mutación}\\
				$NuevaSlcn \leftarrow BusquedaLocal(SlcnMutada,\;MaxLlamadasLS)$\\
				
				\texttt{\#Actualizamos la mejor solución si fuese necesario}\\
				\If{$C(NuevaSlcn) < C(MejorSolucion)$}{
					$MejorSolucion \leftarrow NuevaSlcn$\\
				}
				
			}
			
			\KwRet{$MejorSolucion$}
			
		}
			
	\end{algorithm}
		
\section{Hibridación ILS-ES}

	\noindent Muy similar a ILS, con la diferencia de que, en lugar de utilizar Búsqueda Local para mejorar las soluciones obtenidas mediante el operador de mutación, emplearemos el algoritmo de Enfriamiento Simulado con este fin. A continuación se muestra el pseudocódigo correspondiente a esta idea (la función $C$ es la función de coste):

	\begin{algorithm}
		
		\textbf{function} $ILS-ES(MaxIters,\;MaxLlamadasLS,\;FraccSublista\;...)$
		\Begin{
			\texttt{\#Inicializamos la mejor solución de forma aleatoria}\\
			$MejorSolucion \leftarrow GenerarSolucionAleatoria()$\\
			\vspace{0.2cm}
			\For{$i \leftarrow 0$ \KwTo $MaxIters$}{
				\texttt{\#Generamos una nueva solución de forma aleatoria}\\
				$SlcnMutada \leftarrow OpMutSblstAleatoria(\&MejorSolucion,\;FraccSublista)$\\
				\texttt{\#Lanzamos BL sobre la nueva solución obtenida con el operador de mutación}\\
				$EsSlcn \leftarrow EnfriamientoSimulado(SlcnMutada,\;MaxLlamadasLS\;...)$\\
				
				\texttt{\#Actualizamos la mejor solución si fuese necesario}\\
				\If{$C(NuevaSlcn) < C(MejorSolucion)$}{
					$MejorSolucion \leftarrow NuevaSlcn$\\
				}
				
			}
			
			\KwRet{$MejorSolucion$}
			
		}
		
	\end{algorithm}
	
\section{Procedimiento considerado para el desarrollo de la práctica}

\noindent El código empleado para el desarrollo de esta obra ha sido enteramente escrito por el autor de la misma sin más ayuda que la proporcionada por el guión y los seminarios impartidos en clase, así como la de la documentación del lenguaje \texttt{C++}.\\

\noindent El estilo de programación es orientado a objetos; cabe destacar el uso de bibliotecas implementadas para \texttt{C++} como la biblioteca \texttt{STL}, la biblioteca \texttt{Algorithm} o la biblioteca \texttt{Chrono}, que proporciona un reloj de alta resolución.\\

\section{Experimentos y Análisis de resultados}

	\subsection{Descripción de parámetros y casos considerados}
	
	\noindent Para la obtención de resultados se han considerado todas las instancias proporcionadas para el desarrollo de la práctica, así como los parámetros recomendados en el guión.\\
	
	\noindent Para tomar las mediciones se ha lanzado cada algoritmo para cada instancia 5 veces con 5 semillas diferentes, a saber: 5, 17, 281, 881 y 6673.\\
	
	\subsection{Análisis de resultados del algoritmo Greedy}
	
	\noindent Los resultados obtenidos con el algoritmo Greedy para cada instancia son los siguientes:\\
	
	\begin{table}[!h]
		\centering
		\setlength{\arrayrulewidth}{1mm}
		\setlength{\tabcolsep}{10pt}
		\renewcommand{\arraystretch}{1}
		
		\rowcolors{2}{gray!25}{white}
		\begin{tabular}{ >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{2cm}   >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.6cm}  >{\centering\arraybackslash}m{2cm}  }
			\hline
			\rowcolor{black}
			\multicolumn{6}{c}{\bf \color{white}{Algoritmo Greedy}}\\
			\hline
			\rowcolor{gray!50}
			\textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} & \textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} \\
			chr20b & 365.796 & 8.1752e-06 & sko100a & 13.2327 & 5.26428e-05 \\
			chr22a & 119.916  & 8.6274e-06  & sko100b  & 13.4902  & 6.00204e-05 \\
			els19 & 124.416  & 6.999e-06  & sko100c  & 14.5271  & 5.23372e-05 \\
			esc32b & 90.4762  & 9.674e-06  & sko100d  & 12.5287  & 5.39372e-05 \\
			kra30b & 29.6106  & 6.907e-06  & sko100e  &  13.2511 & 5.32086e-05 \\
			lipa90b & 29.0592  & 4.52854e-05  & tai30  & 117.729 & 6.6426e-06 \\
			nug25 & 18.5363 & 5.0162e-06  & tai50  & 71.8325  & 1.636e-05 \\
			sko56 & 19.2931  & 1.885e-05  & tai60  & 15.8156  & 2.1938e-05 \\
			sko64 & 17.6255  &  2.3584e-05 & tai256  & 120.481  & 0.000295434 \\
			sko72 & 15.6424  & 2.96366e-05  & tho150  & 17.14  & 0.00011395 \\
			\hline
			
		\end{tabular}
		
		\caption{Tabla que contiene los datos asociados al algoritmo Greedy}
		
	\end{table}
	
	\noindent Teniendo en cuenta el reducido orden de complejidad del algoritmo Greedy, así como la simplicidad de las operaciones que realiza, es de esperar que, tal y como sucede, el algoritmo Greedy proporcione resultados excelentes en cuanto al tiempo se refiere; esta mejora en tiempo es a costa de una pérdida notable de calidad en las soluciones. En ningún caso la desviación típica proporcionada por este algoritmo es menor que 10, por tanto, aunque el algoritmo Greedy proporciona un buen marco de comparación, no es el adecuado para resolver este problema si lo que queremos es obtener una solución lo más cercana a la óptima posible. 
	
	\FloatBarrier
	
	\subsection{Análisis de resultados de la Búsqueda Local}
	
	\noindent Los resultados obtenidos con la Búsqueda Local para cada instancia son los siguientes:\\
	
	\begin{table}[!h]
		\centering
		\setlength{\arrayrulewidth}{1mm}
		\setlength{\tabcolsep}{10pt}
		\renewcommand{\arraystretch}{1.1}
		
		\rowcolors{2}{gray!25}{white}
		\begin{tabular}{ >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{2cm}   >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.6cm}  >{\centering\arraybackslash}m{2cm}  }
			\hline
			\rowcolor{black}
			\multicolumn{6}{c}{\bf \color{white}{Algoritmo de Búsqueda Local}}\\
			\hline
			\rowcolor{gray!50}
			\textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} & \textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} \\
			chr20b & 49.121 & 0.000124205  & sko100a  & 2.10892  & 0.0354537  \\
			chr22a & 13.9766  & 0.000199907 & sko100b  &  1.98271 & 0.0334663  \\
			els19 &  35.0829  & 0.000204955  & sko100c  & 1.6345  & 0.0413691  \\
			esc32b & 30  & 0.000607973  & sko100d  & 1.78852  & 0.0342172  \\
			kra30b & 5.88711  &  0.000605045 & sko100e  & 1.81026 & 0.0305345  \\
			lipa90b & 21.5846  & 0.0264895  & tai30  & 18.0639  & 0.000691034  \\
			nug25 & 5.26709  & 0.00029219  & tai50  & 7.09251  & 0.00378619  \\
			sko56 & 2.43891  & 0.00551328  & tai60  & 4.3088  & 0.006036  \\
			sko64 & 1.99183  & 0.00905081  & tai256  & 0.385985  & 0.490289  \\
			sko72 & 2.58573  & 0.0119419  & tho150  & 1.92728  & 0.19621  \\
			\hline
			
		\end{tabular}
		
		\caption{Tabla que contiene los datos asociados a la Búsqueda Local}
		
	\end{table}
	
	\noindent La búsqueda local es un método de resolución de problemas eficiente en cuanto a tiempo se refiere, pero siempre corre el riesgo de caer en óptimos locales y nunca alcanzar la solución óptima, es más, podría darse el caso de que el algoritmo cayera en un óptimo local muy alejado de la solución óptima, tal y como parece suceder en los casos de menor dimensión. Por otra parte vemos que la búsqueda local es capaz de proporcionar desviaciones respecto al óptimo inferiores incluso al 1\%. De esta forma podemos decir que la búsqueda local parece adecuada si tenemos información sobre los datos que nos permita concluir que la probabilidad de caer en un óptimo local es pequeña.\\

	\FloatBarrier
	
	\subsection{Análisis de resultados del Enfriamiento Simulado}
	
	\begin{table}[h]
		\centering
		\setlength{\arrayrulewidth}{1mm}
		\setlength{\tabcolsep}{10pt}
		\renewcommand{\arraystretch}{1.1}
		
		\rowcolors{2}{gray!25}{white}
		\begin{tabular}{ >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{2cm}   >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.6cm}  >{\centering\arraybackslash}m{2cm}  }
			\hline
			\rowcolor{black}
			\multicolumn{6}{c}{\bf \color{white}{Algoritmo de Enfriamiento Simulado}}\\
			\hline
			\rowcolor{gray!50}
			\textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} & \textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} \\
			chr20b & 42.2802 & 0.00152593 & sko100a  &  1.78234 & 0.0138806 \\
			chr22a & 14.3665 & 0.00197728 & sko100b  & 2.04068 & 0.0144491 \\
			els19 & 34.624 & 0.00412297 & sko100c  & 2.7247 & 0.0135785 \\
			esc32b & 13.8095 & 0.00143775 & sko100d  & 1.91849 & 0.0143158 \\
			kra30b & 5.70116 & 0.00410259 & sko100e  & 2.4 & 0.014177 \\
			lipa90b & 22.3519 & 0.0055648 & tai30  & 11.4635 & 0.000528993 \\
			nug25 & 3.19444 & 0.000275109 & tai50  & 5.885 & 0.00193798 \\
			sko56 & 3.70654 & 0.00222522 & tai60  & 5.33505 & 0.00202294 \\
			sko64 & 3.45334 & 0.00369136 & tai256  & 5.35515 & 0.00195059 \\
			sko72 & 2.92502 & 0.00486041 & tho150  & 3.5706 & 0.019035 \\
			\hline
			
		\end{tabular}
		
		\caption{Tabla que contiene los datos asociados al algoritmo de Enfriamiento Simulado}
		
	\end{table}
	
	\noindent El algoritmo de enfriamiento simulado resulta eficiente respecto al tiempo, concretamente obtiene mejores resultados que la búsqueda local en lo que a esta medida se refiere. Por otra parte, parece obtener resultados que mejoran a la búsqueda local en los casos de menor dimensión, mientras que en los casos de mayor dimensión es la búsqueda local la que obtiene la ventaja. El algoritmo de ES parece no ser el adecuado para resolver los casos de mayor dimensión, aun así obtiene resultados excelentes respecto al tiempo.
	
	\FloatBarrier\clearpage
	
	\subsection{Análisis de resultados de la Búsqueda Multiarranque Básica}
	
	\begin{table}[h]
		\centering
		\setlength{\arrayrulewidth}{1mm}
		\setlength{\tabcolsep}{10pt}
		\renewcommand{\arraystretch}{1.1}
		
		\rowcolors{2}{gray!25}{white}
		\begin{tabular}{ >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{2cm}   >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.6cm}  >{\centering\arraybackslash}m{2cm}  }
			\hline
			\rowcolor{black}
			\multicolumn{6}{c}{\bf \color{white}{Algoritmo de Búsqueda Multiarranque Básica}}\\
			\hline
			\rowcolor{gray!50}
			\textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} & \textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} \\
			chr20b & 21.114 & 0.00336642 & sko100a  & 0.973671 & 1.01539 \\
			chr22a & 6.36777 & 0.00490697 & sko100b  & 1.13276 & 0.957014 \\
			els19 & 3.85076 & 0.00393425 & sko100c  & 1.15378 & 0.958829 \\
			esc32b & 11.9048 & 0.0141487 & sko100d  & 1.11756 & 0.952573 \\
			kra30b & 2.028 & 0.0137506 & sko100e  & 1.13711 & 0.979864 \\
			lipa90b & 21.207 & 0.633258 & tai30  & 1.23248 & 0.0239779 \\
			nug25 & 0.747863 & 0.00802006 & tai50  & 1.37149 & 0.0992864 \\
			sko56 & 1.4998 & 0.12673 & tai60  & 3.53464 & 0.159128 \\
			sko64 & 1.34603 & 0.213188 & tai256  & 0.286678 & 13.5233 \\
			sko72 & 1.30162 & 0.306443 & tho150  & 1.33283 & 4.24885 \\
			\hline
			
		\end{tabular}
		
		\caption{Tabla que contiene los datos asociados al algoritmo de Búsqueda Multiarranque Básica}
		
	\end{table}
		
	\noindent La búsqueda multiarranque básica obtiene soluciones al problema en menos de un segundo excepto para los casos de mayor dimensión, por tanto podemos decir que resulta eficiente en lo que al tiempo se refiere. Por otra parte, como es de esperar, mejora a la búsqueda local respecto a la calidad de las soluciones obtenidas. Observamos una estrecha relación entre BL y BMB, ya que esta segunda consiste en lanzar varias veces la primera, por tanto el tiempo que BMB emplea en obtener la solución es proporcional al empleado por BL; es gracias a este incremento en el tiempo de ejecución que BMB obtiene mejores resultados de BL.
		
	\FloatBarrier
	
	\subsection{Análisis de resultados GRASP}
	
	\begin{table}[h]
		\centering
		\setlength{\arrayrulewidth}{1mm}
		\setlength{\tabcolsep}{10pt}
		\renewcommand{\arraystretch}{1.1}
		
		\rowcolors{2}{gray!25}{white}
		\begin{tabular}{ >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{2cm}   >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.6cm}  >{\centering\arraybackslash}m{2cm}  }
			\hline
			\rowcolor{black}
			\multicolumn{6}{c}{\bf \color{white}{Algoritmo GRASP}}\\
			\hline
			\rowcolor{gray!50}
			\textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} & \textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} \\
			chr20b & 19.0775 & 0.00545148 & sko100a  & 0.970514 & 1.54063 \\
			chr22a & 6.66017 & 0.00811693 & sko100b  & 1.03373 & 1.54228  \\
			els19 & 3.54704 & 0.0057894 & sko100c  & 1.05152 & 1.57772 \\
			esc32b & 12.8571 & 0.0235059 & sko100d  & 1.10205 & 1.55468 \\
			kra30b & 2.26427 & 0.0218917 & sko100e  & 1.16071 & 1.564 \\
			lipa90b & 21.131 & 1.03603 & tai30  & 0.436801 & 0.025703 \\
			nug25 & 1.13248 & 0.0123982 & tai50  & 1.05046 & 0.145528 \\
			sko56 & 1.27924 & 0.199471 & tai60  & 3.55004 & 0.247251 \\
			sko64 & 1.40294 & 0.302667 & tai256  & 0.312504 & 39.2974 \\
			sko72 & 1.2986 & 0.466713 & tho150  & 1.3544 & 6.80024 \\
			\hline
			
		\end{tabular}
		
		\caption{Tabla que contiene los datos asociados al algoritmo GRASP}
		
	\end{table}
	
	\noindent El algoritmo GRASP es capaz de obtener soluciones menos condicionadas a los óptimos locales que todos los considerados anteriormente en este documento. Esta mejora, claro está, implica un incremento en el tiempo de ejecución que se hace más notable a medida que incrementamos la dimensión del problema. Aún con todo, GRASP no parece ser el método adecuado para la resolución del problema QAP, puesto que en la mayoría de las ocasiones las mejoras obtenidas respecto al resto de algoritmos es a costa de un sobrecoste en el tiempo que no es proporcional a la mejora obtenida.
	
	\FloatBarrier
	
	\subsection{Análisis de resultados de la Búsqueda Local Reiterada}
	
	\begin{table}[h]
		\centering
		\setlength{\arrayrulewidth}{1mm}
		\setlength{\tabcolsep}{10pt}
		\renewcommand{\arraystretch}{1.1}
		
		\rowcolors{2}{gray!25}{white}
		\begin{tabular}{ >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{2cm}   >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.6cm}  >{\centering\arraybackslash}m{2cm}  }
			\hline
			\rowcolor{black}
			\multicolumn{6}{c}{\bf \color{white}{Algoritmo de Búsqueda Local Reiterada}}\\
			\hline
			\rowcolor{gray!50}
			\textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} & \textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} \\
			chr20b & 17.5457 & 0.0026577 & sko100a  & 0.491046 & 0.85334 \\
			chr22a & 6.62118 & 0.00423482 & sko100b  & 0.579895 & 0.825253 \\
			els19 & 6.14337 & 0.0029313 & sko100c  & 0.76639 & 0.768706 \\
			esc32b & 11.4286 & 0.0100749 & sko100d  & 0.868856 & 0.837173 \\
			kra30b & 1.48108 & 0.0104997 & sko100e  & 0.762186 & 0.78429 \\
			lipa90b & 21.104 & 0.60355 & tai30  & 1.49411 & 0.0141493 \\
			nug25 & 0.459402 & 0.00622479 & tai50  & 1.08169 & 0.0831006 \\
			sko56 & 1.16664 & 0.110512 & tai60  & 3.11625 & 0.136273 \\
			sko64 & 0.780238 & 0.17724 & tai256  & 0.290796 & 7.93192 \\
			sko72 & 0.827095 & 0.267107 & tho150  & 0.891662 & 3.59569 \\
			\hline
			
		\end{tabular}
		
		\caption{Tabla que contiene los datos asociados al algoritmo de Búsqueda Local Reiterada}
		
	\end{table}
	
	\noindent El algoritmo de búsqueda local reiterada es el que obtiene mejores resultados, en lo que a calidad de las soluciones se refiere, de entre todos los considerados en este documento. En cuanto al tiempo vemos que, en general, mejora a BMB, que era el método que hasta ahora obtenía mejores resultados. Por tanto parece ser éste el método adecuado para resolver el problema QAP en cuanto lo que desviación respecto al óptimo se refiere.
	
	\FloatBarrier
	
	\subsection{Análisis de resultados del híbrido ILS-LS}
	
	\begin{table}[h]
		\centering
		\setlength{\arrayrulewidth}{1mm}
		\setlength{\tabcolsep}{10pt}
		\renewcommand{\arraystretch}{1.1}
		
		\rowcolors{2}{gray!25}{white}
		\begin{tabular}{ >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{2cm}   >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.6cm}  >{\centering\arraybackslash}m{2cm}  }
			\hline
			\rowcolor{black}
			\multicolumn{6}{c}{\bf \color{white}{Algoritmo híbrido ILS-ES}}\\
			\hline
			\rowcolor{gray!50}
			\textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} & \textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} \\
			chr20b & 17.5979 & 0.0663293 & sko100a  & 0.74578 & 0.285966 \\
			chr22a & 6.13385 & 0.0377989 & sko100b  & 0.753005 & 0.281156 \\
			els19 & 15.0306 & 0.0929369 & sko100c  & 0.822929 & 0.278866 \\
			esc32b & 0 & 0.0586142 & sko100d  & 0.913516 & 0.277938 \\
			kra30b & 1.21418 & 0.125549 & sko100e  & 0.950989 & 0.294719 \\
			lipa90b & 21.5952 & 0.142827 & tai30  & 2.59364 & 0.00947199 \\
			nug25 & 0.512821 & 0.00915123 & tai50  & 1.54013 & 0.0432994 \\
			sko56 & 1.18521 & 0.0569492 & tai60  & 4.15979 & 0.0468222 \\
			sko64 & 0.848695 & 0.0792185 & tai256  & 2.35326 & 0.051929 \\
			sko72 & 0.919464 & 0.112909 & tho150  & 0.965382 & 0.794868 \\
			\hline
			
		\end{tabular}
		
		\caption{Tabla que contiene los datos asociados al algoritmo híbrido ILS-ES}
		
	\end{table}
	
	\noindent El híbrido ILS-ES consiste en aplicar ES como método de mejora para las soluciones obtenidas mediante el operador de mutación de ILS, por tanto, es lógico pensar que las diferencias entre ES y BL se verán reflejadas en este algoritmo. Vemos que, como esperábamos, este algoritmo mejora en tiempo a ILS, no es así respecto a la calidad de las soluciones que, aunque peores que las obtenidas con ILS, siguen siendo soluciones cercanas a la óptima.
	
	\FloatBarrier
	
	\subsection{Análisis de resultados generales}
	
	\noindent La siguiente tabla recoge los resultados medios obtenidos para cada algoritmo:
	
	\begin{table}[h]
		\centering
		\setlength{\arrayrulewidth}{1mm}
		\setlength{\tabcolsep}{10pt}
		\renewcommand{\arraystretch}{1.3}
		
		\rowcolors{2}{gray!25}{white}
		\begin{tabular}{ >{\centering\arraybackslash}m{3cm}  >{\centering\arraybackslash}m{2cm}  >{\centering\arraybackslash}m{2.7cm} }
			\hline
			\rowcolor{black}
			\textbf{\color{white} Algoritmo} & \textbf{\color{white} Desv} & \textbf{\color{white} Tiempo} \\
			Greedy & 62,01996 & 4,481808E-05 \\
			BL & 10,45195775 & 0,0467831012 \\
			ES & 9,4444055 & 0,0065521532 \\
			BMB & 4,2320321 & 1,1932433035 \\
			GRASP & 4,13365345 & 2,865633258 \\
			ILS & 3,8950093 & 0,824777754 \\
			ILS-ES & 4,04181705 & 0,1601281345 \\
			\hline
			
		\end{tabular}
		
		\caption{Tabla que contiene los datos asociados al análisis de resultados medios generales}
		
	\end{table}
	
	\noindent Tras el análisis detallado de cada uno de los algoritmos considerados en este documento, analizamos la media de los resultados obtenidos por cada uno de ellos. Tal y como hemos visto en el análisis de ILS, es este algoritmo el que obtiene los mejores resultados respecto a la calidad de las soluciones, por tanto, parece ser el método para evitar la localidad que implementa este algoritmo el más adecuado para el problema de la asignación cuadrática\\
	
	\noindent Respecto al tiempo, podemos comparar BL y ES dada la similaridad en las soluciones obtenidas; vemos que ES obtiene soluciones de calidad similar a BL en tiempo menor, por tanto si es esta última característica la que consideramos como más relevante deberá ser ES el que utilicemos para resolver el problema. Lo mismo sucede al comparar ILS-ES e ILS, siendo esta última la que obtiene mejores resultados en lo que a desviación respecto al óptimo se refiere.
	
	\FloatBarrier
	\clearpage
	
\section{Manual de usuario}

	\noindent Junto al código fuente utilizado para el desarrollo de la práctica se incluye un archivo tipo makefile que automatiza el proceso de compilación; este archivo genera un fichero ejecutable para cada uno de los algoritmos, así como un ejecutable general llamado \texttt{mainGeneral} que lanza todos los algoritmos con los parámetros especificados en el guión, es este último el utilizado para tomar las mediciones pertinentes. Además, se incorpora un archivo \texttt{run.sh} que ejecutará el binario \texttt{mainGeneral} para cada una de las instancias del problema y almacenará el resultado en el directorio \texttt{results} bajo el nombre de \texttt{<nombre\_instancia>.rslt}.
	

\end{document}

\grid

		\begin{table}[h]
			\centering
			\setlength{\arrayrulewidth}{1mm}
			\setlength{\tabcolsep}{10pt}
			\renewcommand{\arraystretch}{1.5}
			
			\rowcolors{2}{gray!25}{white}
			\begin{tabular}{ >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{2cm}   >{\centering\arraybackslash}m{1.3cm}  >{\centering\arraybackslash}m{1.6cm}  >{\centering\arraybackslash}m{2cm}  }
				\hline
				\rowcolor{black}
				\multicolumn{6}{c}{\bf \color{white}{Algoritmo Greedy}}\\
				\hline
				\rowcolor{gray!50}
				\textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} & \textbf{Caso} & \textbf{Desv} & \textbf{Tiempo} \\
				chr20b &   &   & sko100a  &   &   \\
				chr22a &   &   & sko100b  &   &   \\
				els19 &   &   & sko100c  &   &   \\
				esc32b &   &   & sko100d  &   &   \\
				kra30b &   &   & sko100e  &   &   \\
				lipa90b &   &   & tai30  &   &   \\
				nug25 &   &   & tai50  &   &   \\
				sko56 &   &   & tai60  &   &   \\
				sko64 &   &   & tai256  &   &   \\
				sko72 &   &   & tho150  &   &   \\
				\hline
				
			\end{tabular}
			
		\end{table}